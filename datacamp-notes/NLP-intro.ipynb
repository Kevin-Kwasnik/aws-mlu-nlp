{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31496ad1",
   "metadata": {},
   "source": [
    "## I. Regular Expressions (Regex)\n",
    "\n",
    "- Strings with a special syntax \n",
    "- Allow us to match patterns in other strings\n",
    "- Applications:\n",
    "    - find weblinks in a document\n",
    "    - parse email addresses\n",
    "    - remove/replace unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c12441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11173153",
   "metadata": {},
   "source": [
    "### 1. Reference\n",
    "\n",
    "|pattern | matches | example |\n",
    "| :- | :- | :- |\n",
    "| \\w+| word | 'Magic' |\n",
    "| \\d | digit | 9 |\n",
    "| \\s | space | '' |\n",
    "| .* | wildcard | 'username74' |\n",
    "| + or * | greedy match | 'aaaaaa' |\n",
    "| \\S| **not** space | 'no_spaces' |\n",
    "| [a-z]| lowercase group | 'abcdefg' |\n",
    "| [A-Z]| uppercase group | 'ABCDEFG' |\n",
    "| [.?!]| symbol group | '.' or '?'|\n",
    "|[A-Za-a] | upper and lowercase English alphabet | \"ABCDEFghijk\" |\n",
    "| [0-9] | numbers from 0 to 9 | 9 |\n",
    "| [A-Za-z\\-\\.] | upper and lowercase English alphabet, - and . | 'My-Website.com' |\n",
    "| (a-z) | a, - and z | 'a-z' |\n",
    "| (\\s+|,) | spaces or a comma | ',' |\n",
    "\n",
    "> **note**: since `-` and `.` are special characters in regex, to look for them explicitly an escape character `\\` is needed directly before the character. \n",
    "\n",
    "**example** find anything in square brackets:\n",
    "```python\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "```\n",
    "#### Regex using or \"I\"\n",
    "- OR is represented using `|`\n",
    "- You can define a group using `()`\n",
    "> only what is defined explicitly is matched\n",
    "- You can define explicit character ranges using `[]`\n",
    "\n",
    "**example** find any words or digits:\n",
    "```python\n",
    "match_digits_and_words = r\"(\\d+|\\w+)\"\n",
    "```\n",
    "### 2. re methods\n",
    "- <span style= \"color:indianred\">Pattern first, string second </span>\n",
    "- May return an iterator, string, or match object\n",
    "\n",
    "#### match()\n",
    "- matches a pattern with a string, taking pattern as first arg and string as second and returns match oobject.\n",
    "\n",
    "- note: using symbols as capital negates them\n",
    "\n",
    "#### search()\n",
    "- search for a pattern\n",
    "\n",
    "> <span style= \"color:royalblue\"> **NOTE** on search vs. match: `search` will go through the ENTIRE string to look for match options, while `match` tries to match from the beginning of a string until it can no longer match.</span> <br>\n",
    "<span style= \"color:indianred\"> If you need to find a pattern that might not be at the beginning of the string, you should use search. If you want to be specific about the composition of the entire string, or at least the initial pattern, then you should use match.</span>\n",
    "\n",
    "#### split()\n",
    "- split a string on a regex\n",
    "\n",
    "**e.g.:**\n",
    "```python\n",
    "re.split('\\s+', 'Split on spaces.')\n",
    "# would return:\n",
    "['Split', 'on', 'spaces.']\n",
    "```\n",
    "> *This can be used for tokenization, so you can preprocess text using regex*\n",
    "\n",
    "#### findall()\n",
    "- find all patterns in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8449ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE:\n",
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73e8eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n",
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6978415",
   "metadata": {},
   "source": [
    "## II. Tokenization\n",
    "- <span style= \"color:indianred\">Pattern first, string second </span>\n",
    "\n",
    "#### overview\n",
    "- Transforming a string or document into tokens(smaller chunks)\n",
    "- One step in preparing a text for NLP\n",
    "- Many different theories and rules\n",
    "- You can create your own rules using regular expressions\n",
    "- Examples:\n",
    "    - breaking out words or sentences\n",
    "    - separating punctuation\n",
    "    - spearating parts, such as all hashtags in a tweet\n",
    "  \n",
    "#### value of tokenization\n",
    "- Easier to match part of speech\n",
    "- Matching common words\n",
    "- Removing unwanted tokens\n",
    "> e.g.: \"I don't like Sam's shoes.\" <br>\n",
    "reveals **negation** in `\"n't\"` and **possession** in `\"'s\"` in the following output: <br>\n",
    "```python\n",
    "[\"I\", \"do\",\"n't\",\"like\",\"Sam\",\"'s\",\"shoes\",\".\"]\n",
    "```\n",
    "  \n",
    "### nltk library\n",
    "`nltk`: natural language toolkit\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "work_tokenize(\"Hi there!\")\n",
    "```\n",
    "would output: \n",
    "```python\n",
    "['Hi', 'there', '!']\n",
    "```\n",
    "\n",
    "#### other nltk tokenizers\n",
    "\n",
    "`sent_tokenize`: tokenize a document into sentences <br>\n",
    "`regexp_tokenize`: tokenize a string or document based on a regular expression pattern <br>\n",
    "`TweetTokenizer`: special class for tweet tokenization, allows separation of hashtags, mentions and lots of exclamation points!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e40ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"SOLDIER #1: Found them? In Mercea? The coconut's tropical!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba81c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd405b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SOLDIER',\n",
       " '#1',\n",
       " 'Found',\n",
       " 'them',\n",
       " '?',\n",
       " 'In',\n",
       " 'Mercea',\n",
       " '?',\n",
       " 'The',\n",
       " 'coconut',\n",
       " 's',\n",
       " 'tropical',\n",
       " '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tokenize(my_string, r\"(\\w+|#\\d|\\?|!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c53f1d",
   "metadata": {},
   "source": [
    "#### how to find mentions and hashtags:\n",
    "```python\n",
    "r\"([@#]\\w+)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474a25d",
   "metadata": {},
   "source": [
    "#### How to tokenize tweets\n",
    "```python\n",
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781f4b5",
   "metadata": {},
   "source": [
    "### III. Bag-of-Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec5b5a",
   "metadata": {},
   "source": [
    "- Basic method for finding topics in a text\n",
    "- First create tokens using tokenization\n",
    "- ... then count tokens\n",
    "- Can be a good way to identify word significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f64181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 2, 'cat': 2, 'is': 1, 'in': 1, 'the': 1, 'box': 2, '.': 2})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "counted_words = Counter(word_tokenize(\"\"\"The cat is in the box. The cat box.\"\"\"))\n",
    "counted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d21b31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 2), ('cat', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counted_words.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9a2478",
   "metadata": {},
   "source": [
    "### IV. Simple Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "392c0ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer and stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "text = \"\"\"The cat is in the box. The cat likes the box.\n",
    "                The box is over the cat\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6cd88e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension to tokenize\n",
    "# isalpha tests if it only alphabetic characters\n",
    "lower_tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4858b3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 3), ('box', 3), ('like', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e59704",
   "metadata": {},
   "source": [
    "### V. Gensim intro\n",
    "\n",
    "- Popular open-source NLP library\n",
    "- Uses top academic models to perform complex tasks\n",
    "    - building document or word vectors\n",
    "    - performing topic identification and documnet comparison\n",
    "- `gensim` models can be easily saved, updated, and reused\n",
    "- dicts can also be updated\n",
    "- The immediate below is a more advanced, feature rich bag-of-words.\n",
    "\n",
    "\n",
    "#### LDA visualization\n",
    "- LDA = latent dirichlet allocation\n",
    "- used as part of preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873d5395",
   "metadata": {},
   "source": [
    "#### BOW in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39561252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f43f4e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_docs = [\n",
    "    'The movie was about a spaceship and aliens',\n",
    "    'I really liked the movie!',\n",
    "    'Awesome action scenes, but boring characters.',\n",
    "    'The movie was awful! I hate alien films.',\n",
    "    'Space is cool! I liked the movie.',\n",
    "    'More space films, please!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc08f676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'about': 1,\n",
       " 'aliens': 2,\n",
       " 'and': 3,\n",
       " 'movie': 4,\n",
       " 'spaceship': 5,\n",
       " 'the': 6,\n",
       " 'was': 7,\n",
       " '!': 8,\n",
       " 'i': 9,\n",
       " 'liked': 10,\n",
       " 'really': 11,\n",
       " ',': 12,\n",
       " '.': 13,\n",
       " 'action': 14,\n",
       " 'awesome': 15,\n",
       " 'boring': 16,\n",
       " 'but': 17,\n",
       " 'characters': 18,\n",
       " 'scenes': 19,\n",
       " 'alien': 20,\n",
       " 'awful': 21,\n",
       " 'films': 22,\n",
       " 'hate': 23,\n",
       " 'cool': 24,\n",
       " 'is': 25,\n",
       " 'space': 26,\n",
       " 'more': 27,\n",
       " 'please': 28}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_docs]\n",
    "dictionary = Dictionary(tokenized_docs) # adding the tokens to a dict\n",
    "dictionary.token2id # how to access the gensim dict of tokens and their ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41a36337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(4, 1), (6, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(4, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (13, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(4, 1), (6, 1), (8, 1), (9, 1), (10, 1), (13, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(8, 1), (12, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### creating a gensim corpus\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "corpus \n",
    "    # first tuple item = token id from dict\n",
    "    # second tuple item = token frequency in doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74ba93",
   "metadata": {},
   "source": [
    "### Tf-idf with gensim\n",
    "\n",
    "- Term frequency - inverse document frequency\n",
    "- Allows you to determine the most important words in each document\n",
    "- Each corpus may have shared words beyond stopwards\n",
    "- These words can be down-weighted in importance\n",
    "- Example from astronomy: \"Sky\"\n",
    "- Ensures most common words don't show up as key words\n",
    "- Keeps document specific frequent words weighted high, but corpus wide as weighted low"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e2da8",
   "metadata": {},
   "source": [
    "#### Tf-idf formula\n",
    "\n",
    "$$ w_{i,j} = tf_{i,j} * log(\\frac{N}{df_{i}}) $$\n",
    "\n",
    "$ w_{i,j} = $ tf-idf weight for token $i$ in documnet $j$\n",
    "<br>\n",
    "<br>\n",
    "$ tf_{i,j} = $ number of occurences of token $i$ in document $j$\n",
    "<br>\n",
    "<br>\n",
    "$ df_{i} = $ number of documents that contain token $i$\n",
    "<br>\n",
    "<br>\n",
    "$N = $ total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a01cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ad75a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 0.1746298276735174),\n",
       " (6, 0.1746298276735174),\n",
       " (8, 0.1746298276735174),\n",
       " (9, 0.29853166221463673),\n",
       " (10, 0.47316148988815415),\n",
       " (11, 0.7716931521027908)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[corpus[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcea65b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
